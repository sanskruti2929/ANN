{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tzFCEAoLhEz"
      },
      "outputs": [],
      "source": [
        "# . Write a python program to show Back Propagation Network for XOR function with Binary Input\n",
        "# and Output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ftM3zwqOLp6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the sigmoid activation function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)"
      ],
      "metadata": {
        "id": "DZd2uHRDMAH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network class\n",
        "class NeuralNetwork:\n",
        "    def __init__(self):\n",
        "        # Initialize weights and biases randomly\n",
        "        self.weights_input_hidden = np.random.rand(2, 2)\n",
        "        self.bias_input_hidden = np.random.rand(1, 2)\n",
        "        self.weights_hidden_output = np.random.rand(2, 1)\n",
        "        self.bias_hidden_output = np.random.rand(1, 1)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        # Forward propagation\n",
        "        self.hidden_output = sigmoid(np.dot(X, self.weights_input_hidden) + self.bias_input_hidden)\n",
        "        self.output = sigmoid(np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_hidden_output)\n",
        "        return self.output\n",
        "\n",
        "    def backpropagation(self, X, y, learning_rate):\n",
        "        # Backpropagation\n",
        "        output_error = y - self.output\n",
        "        output_delta = output_error * sigmoid_derivative(self.output)\n",
        "\n",
        "        hidden_error = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * learning_rate\n",
        "        self.bias_hidden_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += X.T.dot(hidden_delta) * learning_rate\n",
        "        self.bias_input_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        # Training loop\n",
        "        for epoch in range(epochs):\n",
        "            # Forward propagation\n",
        "            output = self.feedforward(X)\n",
        "            # Backpropagation\n",
        "            self.backpropagation(X, y, learning_rate)\n",
        "            # Print the loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f\"Epoch {epoch}: Loss = {loss}\")\n"
      ],
      "metadata": {
        "id": "S_JXq_iqMMzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input data and labels for XOR function\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Create a neural network instance\n",
        "nn = NeuralNetwork()\n",
        "\n",
        "# Train the neural network\n",
        "epochs = 10000\n",
        "learning_rate = 0.1\n",
        "nn.train(X, y, epochs, learning_rate)\n",
        "\n",
        "# Test the trained model\n",
        "test_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "predictions = nn.feedforward(test_data)\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "p6y78KaHMPid",
        "outputId": "b1cc987d-a6a7-4035-fe97-ff17c466a2ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.3744394357673707\n",
            "Epoch 100: Loss = 0.2500738701693103\n",
            "Epoch 200: Loss = 0.25000657997395104\n",
            "Epoch 300: Loss = 0.25000445188729725\n",
            "Epoch 400: Loss = 0.2500022079755617\n",
            "Epoch 500: Loss = 0.24999981368315916\n",
            "Epoch 600: Loss = 0.24999723818582353\n",
            "Epoch 700: Loss = 0.24999444644084856\n",
            "Epoch 800: Loss = 0.24999139838133239\n",
            "Epoch 900: Loss = 0.24998804795717489\n",
            "Epoch 1000: Loss = 0.2499843419871885\n",
            "Epoch 1100: Loss = 0.24998021877875584\n",
            "Epoch 1200: Loss = 0.2499756064604795\n",
            "Epoch 1300: Loss = 0.24997042095922795\n",
            "Epoch 1400: Loss = 0.24996456353495938\n",
            "Epoch 1500: Loss = 0.24995791776351695\n",
            "Epoch 1600: Loss = 0.24995034582764047\n",
            "Epoch 1700: Loss = 0.24994168393764726\n",
            "Epoch 1800: Loss = 0.24993173665285348\n",
            "Epoch 1900: Loss = 0.24992026980921583\n",
            "Epoch 2000: Loss = 0.24990700167314772\n",
            "Epoch 2100: Loss = 0.24989159182982637\n",
            "Epoch 2200: Loss = 0.24987362716860642\n",
            "Epoch 2300: Loss = 0.24985260413827903\n",
            "Epoch 2400: Loss = 0.2498279061982825\n",
            "Epoch 2500: Loss = 0.2497987750734817\n",
            "Epoch 2600: Loss = 0.2497642740126781\n",
            "Epoch 2700: Loss = 0.24972324073713095\n",
            "Epoch 2800: Loss = 0.2496742271311591\n",
            "Epoch 2900: Loss = 0.24961542196970044\n",
            "Epoch 3000: Loss = 0.2495445521201548\n",
            "Epoch 3100: Loss = 0.24945875676915796\n",
            "Epoch 3200: Loss = 0.24935442846452133\n",
            "Epoch 3300: Loss = 0.2492270144154311\n",
            "Epoch 3400: Loss = 0.24907077202821187\n",
            "Epoch 3500: Loss = 0.24887847473988128\n",
            "Epoch 3600: Loss = 0.24864106864032126\n",
            "Epoch 3700: Loss = 0.24834728777503462\n",
            "Epoch 3800: Loss = 0.24798324624685236\n",
            "Epoch 3900: Loss = 0.24753203647259425\n",
            "Epoch 4000: Loss = 0.24697337109597628\n",
            "Epoch 4100: Loss = 0.24628330578233096\n",
            "Epoch 4200: Loss = 0.24543406967284337\n",
            "Epoch 4300: Loss = 0.24439402074686695\n",
            "Epoch 4400: Loss = 0.24312776794052895\n",
            "Epoch 4500: Loss = 0.24159661301431334\n",
            "Epoch 4600: Loss = 0.2397597037339747\n",
            "Epoch 4700: Loss = 0.23757661936943825\n",
            "Epoch 4800: Loss = 0.2350123282189306\n",
            "Epoch 4900: Loss = 0.23204514492404607\n",
            "Epoch 5000: Loss = 0.22867696891719225\n",
            "Epoch 5100: Loss = 0.22494267437630588\n",
            "Epoch 5200: Loss = 0.22091334836263293\n",
            "Epoch 5300: Loss = 0.21668869893649728\n",
            "Epoch 5400: Loss = 0.21237877240662403\n",
            "Epoch 5500: Loss = 0.20808149169977036\n",
            "Epoch 5600: Loss = 0.2038653415360121\n",
            "Epoch 5700: Loss = 0.19976351351605948\n",
            "Epoch 5800: Loss = 0.19577941437167068\n",
            "Epoch 5900: Loss = 0.1918975589819134\n",
            "Epoch 6000: Loss = 0.18809156363777402\n",
            "Epoch 6100: Loss = 0.18432386041650337\n",
            "Epoch 6200: Loss = 0.18053791998704855\n",
            "Epoch 6300: Loss = 0.17664770601558727\n",
            "Epoch 6400: Loss = 0.1725280239620275\n",
            "Epoch 6500: Loss = 0.16800667457031612\n",
            "Epoch 6600: Loss = 0.162859343610122\n",
            "Epoch 6700: Loss = 0.15681245598002397\n",
            "Epoch 6800: Loss = 0.149566677686851\n",
            "Epoch 6900: Loss = 0.1408587907011395\n",
            "Epoch 7000: Loss = 0.13056811155685372\n",
            "Epoch 7100: Loss = 0.11883528232405269\n",
            "Epoch 7200: Loss = 0.10612128623022071\n",
            "Epoch 7300: Loss = 0.09313990113102946\n",
            "Epoch 7400: Loss = 0.08066505495492496\n",
            "Epoch 7500: Loss = 0.06931140155369431\n",
            "Epoch 7600: Loss = 0.05941297250553518\n",
            "Epoch 7700: Loss = 0.051036902882499874\n",
            "Epoch 7800: Loss = 0.04407376978066835\n",
            "Epoch 7900: Loss = 0.03833248011708247\n",
            "Epoch 8000: Loss = 0.03360528542387512\n",
            "Epoch 8100: Loss = 0.029701267452005777\n",
            "Epoch 8200: Loss = 0.026458663547768616\n",
            "Epoch 8300: Loss = 0.02374609353764843\n",
            "Epoch 8400: Loss = 0.021459131725273445\n",
            "Epoch 8500: Loss = 0.01951557324688029\n",
            "Epoch 8600: Loss = 0.017850868245025804\n",
            "Epoch 8700: Loss = 0.016414239420573965\n",
            "Epoch 8800: Loss = 0.015165568561493495\n",
            "Epoch 8900: Loss = 0.014072972218750104\n",
            "Epoch 9000: Loss = 0.013110941064285439\n",
            "Epoch 9100: Loss = 0.012258920343408178\n",
            "Epoch 9200: Loss = 0.01150022781628033\n",
            "Epoch 9300: Loss = 0.010821227127710047\n",
            "Epoch 9400: Loss = 0.010210693701190304\n",
            "Epoch 9500: Loss = 0.009659325739285168\n",
            "Epoch 9600: Loss = 0.009159364866588516\n",
            "Epoch 9700: Loss = 0.00870429996034024\n",
            "Epoch 9800: Loss = 0.00828863442018811\n",
            "Epoch 9900: Loss = 0.007907702093669196\n",
            "Predictions:\n",
            "[[0.08499031]\n",
            " [0.91593051]\n",
            " [0.91600557]\n",
            " [0.09425489]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_e4YOpnrMQfn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}